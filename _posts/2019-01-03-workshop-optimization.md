---
title:  "Optimization"
layout: multitrack
abstract: ""
organizer_url: 
categories:
- dali2019
organizers:
- given: Simon   
  family:  Lacoste-Julien
  url: http://www.iro.umontreal.ca/~slacoste/
  institute: U of Montreal
room: "Grey & Knysna Suite"
show_abstracts: false

talks:
- title: "A SGD Safari"
  author: 
  - family: Rosasco
    given: Lorenzo
    url: http://web.mit.edu/lrosasco/www/
    institute: MIT
  start: "09:30"
  end: "10:00" 
- title: "The Robustness of Training on the Test Set"
  author:
  - given: Benjamin
    family: Recht
    url: https://people.eecs.berkeley.edu/~brecht/
    institute: UC Berkeley
  start: "10:00"
  end: "10:30" 
- title: "SignSGD is communication efficient and byzantine tolerant"
  author:
  - given: Anima
    family: Anankumar
    url: http://tensorlab.cms.caltech.edu/users/anima/
    institute: Caltech
  start: "10:30"
  end: "11:00"   
- title: Coffee
  start: "11:00"
  end: "11:30"

- title: "Function norms and regularization in deep networks"
  author:
  - given: Matthew
    family: Blaschko
    url: http://homes.esat.kuleuven.be/~mblaschk/
    institute: KU Leuven
  start: "11:30"
  end: "12:00" 
- title: "Sufficient decrease is all you need"
  author:
  - given: Fabian
    family: Pedregosa 
    url: http://fa.bianp.net/
    institute: Google Brain Montreal
  start: "12:00"
  end: "12:30"   
- title: Lunch
  start: "13:00"
  end: "14:00"
- title: "Debiasing Our Objective Functions"
  author:
  - given: Sebastian
    family:  Nowozin
    url: http://www.nowozin.net/sebastian/
    institute: MSR Cambridge, UK
  start: "17:00"
  end: "17:30" 
  abstract: "In many cases the objective functions we use in machine learning are expectations over iid data. In other cases they are stochastic approximations that are biased. I will use the field of approximate inference as example of such quantities and highlight that at its heart, the field of approximate inference is about trade-offs between computation and estimation accuracy: when we approximate quantities such as the evidence or posterior expectations no randomness is left and given limitless computation budget all quantities can be evaluated exactly. But given finite computation, how do we select inference methods such that they provide accurate estimates of quantities of interest? In this talk I will argue for a more explicit consideration of bias-variance tradeoffs of common inference methods. In particular, I highlight that current inference methods such as variational inference and Markov Chain Monte Carlo make a particular bias-variance tradeoffs which may be suboptimal for our inferential question at hand. What can we do about this? There is a rich portfolio of methods to change bias-variance tradeoffs in the form of debiasing methods; I will provide a brief overview and demonstrate a number of recent successful applications of these methods to variational inference and stochastic gradient MCMC."
- title: "Extragradient and negative momentum to optimize GANs"
  author:
  - given: Simon 
    family:  Lacoste-Julien
    url: http://www.iro.umontreal.ca/~slacoste/
    institute: U of Montreal
  start: "17:30"
  end: "18:00" 
- title: "TBD"
  author:
  - given: Jacob
    family:  Abernathy
    url: https://www.cc.gatech.edu/~jabernethy9/
    institute: Georgia Tech
  start: "18:00"
  end: "18:30"  
speakers:

---

